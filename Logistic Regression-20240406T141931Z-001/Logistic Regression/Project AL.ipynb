{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "welcome-witch",
   "metadata": {},
   "source": [
    "# Now a project\n",
    "\n",
    "## The problem statement\n",
    "\n",
    "\n",
    "In this project, I try to answer the question that whether or not it will rain tomorrow in Australia. I implement Logistic Regression with Python and Scikit-Learn. \n",
    "\n",
    "\n",
    "To answer the question, I build a classifier to predict whether or not it will rain tomorrow in Australia by training a binary classification model using Logistic Regression. I have used the **Rain in Australia** dataset downloaded from the Kaggle website for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-integer",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "\n",
    "I have used the **Rain in Australia** data set downloaded from the Kaggle website.\n",
    "\n",
    "\n",
    "I have downloaded this data set from the [Kaggle website](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package). The data set can be found at the following url:-\n",
    "\n",
    "\n",
    "This dataset contains daily weather observations from numerous Australian weather stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import aztlan as az\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jupyterthemes import jtplot\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1747bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df.columns\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-bleeding",
   "metadata": {},
   "source": [
    "### Types of variables\n",
    "\n",
    "\n",
    "There are a mixture of categorical and numerical variables in the dataset. Categorical variables have data type object. Numerical variables have data type float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "\n",
    "categorical = [c for c in df.columns if df[c].dtype == 'O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :')\n",
    "\n",
    "for i, c in enumerate(categorical):\n",
    "    print('{:2} → {}'.format(i + 1, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-metallic",
   "metadata": {},
   "source": [
    "### Summary of categorical variables\n",
    "\n",
    "\n",
    "- There is a date variable. It is denoted by `Date` column.\n",
    "\n",
    "\n",
    "- There are 6 categorical variables. These are given by `Location`, `WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` and  `RainTomorrow`.\n",
    "\n",
    "\n",
    "- There are two binary categorical variables - `RainToday` and  `RainTomorrow`.\n",
    "\n",
    "\n",
    "- `RainTomorrow` is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-exhaust",
   "metadata": {},
   "source": [
    "## Explore problems within categorical variables\n",
    "\n",
    "\n",
    "First, I will explore the categorical variables.\n",
    "\n",
    "\n",
    "### Missing values in categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables containing missing values\n",
    "cat_nulls = [c for c in categorical if df[c].isnull().sum() != 0]\n",
    "\n",
    "df[cat_nulls].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-ottawa",
   "metadata": {},
   "source": [
    "### Number of labels: cardinality\n",
    "\n",
    "\n",
    "The number of labels within a categorical variable is known as **cardinality**. A high number of labels within a variable is known as **high cardinality**. High cardinality may pose some serious problems in the machine learning model. So, I will check for high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for cardinality in categorical variables\n",
    "msn = '{:15} contains → {:5} labels'\n",
    "for c in categorical:\n",
    "    print(msn.format(c, len(df[c].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RainTomorrow'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-colonial",
   "metadata": {},
   "source": [
    "### Feature Engineering of Date Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates into datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year from date\n",
    "\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "df['Year'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract month from date\n",
    "\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "df['Month'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract day from date\n",
    "\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "df['Day'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again view the summary of dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original Date variable\n",
    "df.drop('Date', axis=1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-saturn",
   "metadata": {},
   "source": [
    "### Explore Categorical Variables\n",
    "\n",
    "\n",
    "Now, I will explore the categorical variables one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "categorical = [c for c in df.columns if df[c].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :')\n",
    "\n",
    "for i, c in enumerate(categorical):\n",
    "    print('{:2} → {}'.format(i + 1, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in categorical variables \n",
    "df[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-shipping",
   "metadata": {},
   "source": [
    "We can see that `WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` variables contain missing values. I will explore these variables one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-vulnerability",
   "metadata": {},
   "source": [
    "### Explore `Location` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of labels in Location variable\n",
    "print(msn.format('Location', len(df['Location'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in location variable\n",
    "df['Location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in Location variable\n",
    "df['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do One Hot Encoding of Location variable\n",
    "# get k-1 dummy variables after One Hot Encoding \n",
    "# preview the dataset with head() method\n",
    "\n",
    "pd.get_dummies(df.Location, drop_first = True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-madrid",
   "metadata": {},
   "source": [
    "### Explore `WindGustDir` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of labels in WindGustDir variable\n",
    "print(msn.format('WindGustDir', len(df['WindGustDir'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in WindGustDir variable\n",
    "df['WindGustDir'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in WindGustDir variable\n",
    "df['WindGustDir'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WindGustDir'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do One Hot Encoding of WindGustDir variable\n",
    "# get k-1 dummy variables after One Hot Encoding \n",
    "# also add an additional dummy variable to indicate there was missing data\n",
    "# preview the dataset with head() method\n",
    "\n",
    "pd.get_dummies(df['WindGustDir'], drop_first = True, dummy_na = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of 1s per boolean variable over the rows of the dataset\n",
    "# it will tell us how many observations we have for each category\n",
    "\n",
    "pd.get_dummies(df.WindGustDir, drop_first = True, dummy_na = True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-honor",
   "metadata": {},
   "source": [
    "### Explore `WindDir9am` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of labels in WindDir9am variable\n",
    "print(msn.format('WindDir9am', len(df['WindDir9am'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in WindDir9am variable\n",
    "df['WindDir9am'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in WindDir9am variable\n",
    "df['WindDir9am'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WindDir9am'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do One Hot Encoding of WindDir9am variable\n",
    "# get k-1 dummy variables after One Hot Encoding \n",
    "# also add an additional dummy variable to indicate there was missing data\n",
    "# preview the dataset with head() method\n",
    "\n",
    "pd.get_dummies(df['WindDir9am'], drop_first = True, dummy_na = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of 1s per boolean variable over the rows of the dataset\n",
    "# it will tell us how many observations we have for each category\n",
    "\n",
    "pd.get_dummies(df['WindDir9am'], drop_first = True, dummy_na = True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-founder",
   "metadata": {},
   "source": [
    "We can see that there are 10566 missing values in the `WindDir9am` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-decline",
   "metadata": {},
   "source": [
    "### Explore `WindDir3pm` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of labels in WindDir3pm variable\n",
    "print(msn.format('WindDir3pm', len(df['WindDir3pm'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in WindDir3pm variable\n",
    "\n",
    "df['WindDir3pm'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in WindDir3pm variable\n",
    "\n",
    "df['WindDir3pm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WindDir3pm'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do One Hot Encoding of WindDir3pm variable\n",
    "# get k-1 dummy variables after One Hot Encoding \n",
    "# also add an additional dummy variable to indicate there was missing data\n",
    "# preview the dataset with head() method\n",
    "\n",
    "pd.get_dummies(df['WindDir3pm'], drop_first = True, dummy_na = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of 1s per boolean variable over the rows of the dataset\n",
    "# it will tell us how many observations we have for each category\n",
    "\n",
    "pd.get_dummies(df['WindDir3pm'], drop_first = True, dummy_na = True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-cemetery",
   "metadata": {},
   "source": [
    "There are 4228 missing values in the `WindDir3pm` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-multiple",
   "metadata": {},
   "source": [
    "### Explore `RainToday` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of labels in RainToday variable\n",
    "print(msn.format('RainToday', len(df['RainToday'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in WindGustDir variable\n",
    "df['RainToday'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in WindGustDir variable\n",
    "\n",
    "df['RainToday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do One Hot Encoding of RainToday variable\n",
    "# get k-1 dummy variables after One Hot Encoding \n",
    "# also add an additional dummy variable to indicate there was missing data\n",
    "# preview the dataset with head() method\n",
    "\n",
    "pd.get_dummies(df['RainToday'], drop_first = True, dummy_na = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of 1s per boolean variable over the rows of the dataset\n",
    "# it will tell us how many observations we have for each category\n",
    "\n",
    "pd.get_dummies(df['RainToday'], drop_first = True, dummy_na = True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-short",
   "metadata": {},
   "source": [
    "There are 3261 missing values in the `RainToday` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-regression",
   "metadata": {},
   "source": [
    "### Explore Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find numerical variables\n",
    "\n",
    "numerical = [c for c in df.columns if df[c].dtype != 'O']\n",
    "\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "print('The categorical variables are :')\n",
    "\n",
    "for i, n in enumerate(numerical):\n",
    "    print('{:2} → {}'.format(i + 1, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the numerical variables\n",
    "\n",
    "df[numerical].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-ownership",
   "metadata": {},
   "source": [
    "### Summary of numerical variables\n",
    "\n",
    "\n",
    "- There are 16 numerical variables. \n",
    "\n",
    "\n",
    "- These are given by `MinTemp`, `MaxTemp`, `Rainfall`, `Evaporation`, `Sunshine`, `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm`, `Humidity9am`, `Humidity3pm`, `Pressure9am`, `Pressure3pm`, `Cloud9am`, `Cloud3pm`, `Temp9am` and `Temp3pm`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-shakespeare",
   "metadata": {},
   "source": [
    "## Explore problems within numerical variables\n",
    "\n",
    "\n",
    "Now, I will explore the numerical variables.\n",
    "\n",
    "\n",
    "### Missing values in numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables\n",
    "\n",
    "df[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-particular",
   "metadata": {},
   "source": [
    "We can see that all the 16 numerical variables contain missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-paradise",
   "metadata": {},
   "source": [
    "### Outliers in numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw boxplots to visualize outliers\n",
    "fig, ax = plt.subplots(2, 2, figsize = (13, 8))\n",
    "\n",
    "\n",
    "sns.boxplot(data = df, x = 'Rainfall', ax = ax[0, 0])\n",
    "sns.boxplot(data = df, x = 'Evaporation', ax = ax[0, 1])\n",
    "sns.boxplot(data = df, x = 'WindSpeed9am', ax = ax[1, 0])\n",
    "sns.boxplot(data = df, x = 'WindSpeed3pm', ax = ax[1, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-model",
   "metadata": {},
   "source": [
    "### Check the distribution of variables\n",
    "\n",
    "\n",
    "Now, I will plot the histograms to check distributions to find out if they are normal or not. If the variable follows normal distribution, then I will do `zscore` otherwise if they are skewed, I will find IQR (Interquantile range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize = (13, 8))\n",
    "\n",
    "\n",
    "sns.histplot(data = df, x = 'Rainfall', ax = ax[0, 0], kde = True)\n",
    "sns.histplot(data = df, x = 'Evaporation', ax = ax[0, 1], kde = True)\n",
    "sns.histplot(data = df, x = 'WindSpeed9am', ax = ax[1, 0], kde = True)\n",
    "sns.histplot(data = df, x = 'WindSpeed3pm', ax = ax[1, 1], kde = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = az.Outliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.fit(df[numerical], verbose = True); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-infection",
   "metadata": {},
   "source": [
    "## Declare feature vector and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = ['RainTomorrow'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['RainTomorrow'], axis=1)\n",
    "\n",
    "y = df['RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-million",
   "metadata": {},
   "source": [
    "## Split data into separate training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of X_train and X_test\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types in X_train\n",
    "\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display categorical variables\n",
    "categorical = [c for c in X_train.columns if X_train[c].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :')\n",
    "\n",
    "for i, c in enumerate(categorical):\n",
    "    print('{:2} → {}'.format(i + 1, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display numerical variables\n",
    "numerical = [c for c in X_train.columns if X_train[c].dtype != 'O']\n",
    "\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "print('The categorical variables are :')\n",
    "\n",
    "for i, n in enumerate(numerical):\n",
    "    print('{:2} → {}'.format(i + 1, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-flash",
   "metadata": {},
   "source": [
    "### Engineering missing values in numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables in X_train\n",
    "\n",
    "X_train[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables in X_test\n",
    "\n",
    "X_test[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentage of missing values in the numerical variables in training set\n",
    "for col in numerical:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print('{:20} → {}'.format(col, round(X_train[col].isnull().mean(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-exploration",
   "metadata": {},
   "source": [
    "### Assumption\n",
    "\n",
    "\n",
    "When there are outliers in the dataset, we should use median imputation because median imputation is robust to outliers.\n",
    "\n",
    "\n",
    "The imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values in X_train and X_test with respective column median in X_train\n",
    "for d in [X_train, X_test]:\n",
    "    for col in numerical:\n",
    "        col_median = X_train[col].median() #only use the training set\n",
    "        d.loc[:, col].fillna(col_median, inplace = True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again missing values in numerical variables in X_train\n",
    "X_train[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables in X_test\n",
    "X_test[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-terry",
   "metadata": {},
   "source": [
    "Now, we can see that there are no missing values in the numerical columns of training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentage of missing values in the categorical variables in training set\n",
    "X_train[categorical].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables with missing data      \n",
    "for col in categorical:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print('{:20} → {}'.format(col, round(X_train[col].isnull().mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing categorical variables with most frequent value\n",
    "\n",
    "for d in [X_train, X_test]:\n",
    "    d['WindGustDir'].fillna(X_train['WindGustDir'].mode()[0], inplace = True)\n",
    "    d['WindDir9am'].fillna(X_train['WindDir9am'].mode()[0], inplace = True)\n",
    "    d['WindDir3pm'].fillna(X_train['WindDir3pm'].mode()[0], inplace = True)\n",
    "    d['RainToday'].fillna(X_train['RainToday'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_train\n",
    "\n",
    "X_train[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_test\n",
    "\n",
    "X_test[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-pacific",
   "metadata": {},
   "source": [
    "As a final check, I will check for missing values in X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_train\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_test\n",
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-clone",
   "metadata": {},
   "source": [
    "We can see that there are no missing values in X_train and X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-mississippi",
   "metadata": {},
   "source": [
    "### Engineering outliers in numerical variables\n",
    "\n",
    "\n",
    "We have seen that the `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns contain outliers. I will use top-coding approach to cap maximum values and remove outliers from the above variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [X_train, X_test]:\n",
    "    outliers = az.Outliers()\n",
    "    d.loc[:, numerical] = outliers.fit(d.loc[:, numerical], how = 'iqr', verbose = True, impute = 'extremes')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw boxplots to visualize outliers\n",
    "fig, ax = plt.subplots(2, 2, figsize = (13, 8))\n",
    "\n",
    "\n",
    "sns.boxplot(data = X_train, x = 'Rainfall', ax = ax[0, 0])\n",
    "sns.boxplot(data = X_train, x = 'Evaporation', ax = ax[0, 1])\n",
    "sns.boxplot(data = X_train, x = 'WindSpeed9am', ax = ax[1, 0])\n",
    "sns.boxplot(data = X_train, x = 'WindSpeed3pm', ax = ax[1, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw boxplots to visualize outliers\n",
    "fig, ax = plt.subplots(2, 2, figsize = (13, 8))\n",
    "\n",
    "\n",
    "sns.boxplot(data = X_test, x = 'Rainfall', ax = ax[0, 0])\n",
    "sns.boxplot(data = X_test, x = 'Evaporation', ax = ax[0, 1])\n",
    "sns.boxplot(data = X_test, x = 'WindSpeed9am', ax = ax[1, 0])\n",
    "sns.boxplot(data = X_test, x = 'WindSpeed3pm', ax = ax[1, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-ballot",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = []\n",
    "\n",
    "for c in categorical:\n",
    "        ohe.append(pd.get_dummies(X_train[c], drop_first = True))\n",
    "        \n",
    "ohe;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train[numerical], *ohe], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = []\n",
    "\n",
    "for c in categorical:\n",
    "        ohe.append(pd.get_dummies(X_test[c], drop_first = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test[numerical], *ohe], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-adelaide",
   "metadata": {},
   "source": [
    "We now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called `feature scaling`. I will do it as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-three",
   "metadata": {},
   "source": [
    "## 11. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-light",
   "metadata": {},
   "source": [
    "## Max-Min Scaling\n",
    "\n",
    "- How to scale your data to any arbitrary range (typically $[0, 1]$)\n",
    "\n",
    "$$\\tilde{x}_i = \\frac{x_i - \\min x}{\\max x - \\min x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create some data\n",
    "\n",
    "N = 42\n",
    "data = np.log(np.random.rand(N)) * 234 + 934\n",
    "\n",
    "# get min and max\n",
    "dataMin = min(data)\n",
    "dataMax = max(data)\n",
    "\n",
    "# now min-max scale\n",
    "dataS = (data - dataMin) / (dataMax - dataMin)\n",
    "\n",
    "\n",
    "# now plot\n",
    "fig,ax = plt.subplots(1,2,figsize=(13,6))\n",
    "\n",
    "ax[0].plot(1 + np.random.randn(N) / 20, data, 'ks')\n",
    "ax[0].axhline(dataMin, color = 'g', alpha = 0.7, label = 'min = {}'.format(np.round(dataMin, 2)))\n",
    "ax[0].axhline(dataMax, color = 'r', alpha = 0.7, label = 'max = {}'.format(np.round(dataMax, 2)))\n",
    "ax[0].set_xlim([0,2])\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_ylabel('Original data scale')\n",
    "ax[0].set_title('Original data')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(1 + np.random.randn(N) /20 ,dataS,'ks')\n",
    "ax[1].axhline(max(dataS), color = 'r', alpha = 0.7, label = 'max = {}'.format(max(dataS)))\n",
    "ax[1].axhline(min(dataS), color = 'g', alpha = 0.7, label = 'min = {}'.format(min(dataS)))\n",
    "ax[1].set_xlim([0,2])\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_ylabel('Unity-normed data scale')\n",
    "ax[1].set_title('Scaled data')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(13,6))\n",
    "\n",
    "ax[0].violinplot(data)\n",
    "ax[0].axhline(dataMin, color = 'g', alpha = 0.7, label = 'min = {}'.format(np.round(dataMin, 2)))\n",
    "ax[0].axhline(dataMax, color = 'r', alpha = 0.7, label = 'max = {}'.format(np.round(dataMax, 2)))\n",
    "ax[0].set_xlim([0,2])\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_ylabel('Original data scale')\n",
    "ax[0].set_title('Original data')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].violinplot(dataS)\n",
    "ax[1].axhline(max(dataS), color = 'r', alpha = 0.7, label = 'max = {}'.format(max(dataS)))\n",
    "ax[1].axhline(min(dataS), color = 'g', alpha = 0.7, label = 'min = {}'.format(min(dataS)))\n",
    "ax[1].set_xlim([0,2])\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_ylabel('Unity-normed data scale')\n",
    "ax[1].set_title('Scaled data')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-hierarchy",
   "metadata": {},
   "source": [
    "We now have `X_train` dataset ready to be fed into the Logistic Regression classifier. I will do it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-steam",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "clf = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-archives",
   "metadata": {},
   "source": [
    "## Predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-mountain",
   "metadata": {},
   "source": [
    "### predict_proba method\n",
    "\n",
    "\n",
    "**predict_proba** method gives the probabilities for the target variable(0 and 1) in this case, in array form.\n",
    "\n",
    "`0 is for probability of no rain` and `1 is for probability of rain.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of getting output as 0 - no rain\n",
    "\n",
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-newport",
   "metadata": {},
   "source": [
    "## Check accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test == y_pred_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-western",
   "metadata": {},
   "source": [
    "Here, **y_test** are the true class labels and **y_pred_test** are the predicted class labels in the test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-treat",
   "metadata": {},
   "source": [
    "### Compare the train-set and test-set accuracy\n",
    "\n",
    "\n",
    "Now, I will compare the train-set and test-set accuracy to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = clf.predict(X_train)\n",
    "\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-myrtle",
   "metadata": {},
   "source": [
    "### Check for overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(clf.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-orange",
   "metadata": {},
   "source": [
    "Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-reservoir",
   "metadata": {},
   "source": [
    "The confusion matrix shows `20892 + 3285 = 24177 correct predictions` and `3087 + 1175 = 4262 incorrect predictions`.\n",
    "\n",
    "\n",
    "In this case, we have\n",
    "\n",
    "\n",
    "- `True Positives` (Actual Positive:1 and Predict Positive:1) - 20892\n",
    "\n",
    "\n",
    "- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 3285\n",
    "\n",
    "\n",
    "- `False Positives` (Actual Negative:0 but Predict Positive:1) - 1175 `(Type I error)`\n",
    "\n",
    "\n",
    "- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 3087 `(Type II error)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "cm_matrix = pd.DataFrame(data = cm, columns=['Real: 1', 'Real: 0'], \n",
    "                                 index=['Predict: 1', 'Predict: 0'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot =True, cmap='magma', fmt='d');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-housing",
   "metadata": {},
   "source": [
    "### Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-conversation",
   "metadata": {},
   "source": [
    "### Classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification accuracy\n",
    "\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-theme",
   "metadata": {},
   "source": [
    "### Classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification error\n",
    "\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-cruise",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "\n",
    "**Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). \n",
    "\n",
    "\n",
    "So, **Precision** identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print precision score\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-princess",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "\n",
    "Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes.\n",
    "It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). **Recall** is also called **Sensitivity**.\n",
    "\n",
    "\n",
    "**Recall** identifies the proportion of correctly predicted actual positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-nomination",
   "metadata": {},
   "source": [
    "## k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.model_selection import cross_val_score, GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becfe17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([int(i == 'Yes') for i in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_train, np.array([int(i == 'Yes') for i in y_train]), cv = 5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_train, np.array([int(i == 'Yes') for i in y_train]), cv = 5, scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_train, y_train, cv = 5, scoring='precision').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1431e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_train, y_train, cv = 5, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26871bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_test, pos_label = 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d24e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
