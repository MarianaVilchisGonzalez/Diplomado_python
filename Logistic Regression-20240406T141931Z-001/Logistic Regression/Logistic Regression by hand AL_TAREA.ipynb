{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "committed-nylon",
   "metadata": {},
   "source": [
    "# What is Linear Classification?\n",
    "\n",
    "Imagine that you have two point clouds that you want to classify, what solution do you propose? The idea that logistic regression proposes is to separate them with a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intelligent-banking",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18152/3410002149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minteract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dprocessing'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "import dprocessing as dp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aggregate-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterthemes import jtplot\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expanded-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dressed-plumbing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.759367</td>\n",
       "      <td>3.450856</td>\n",
       "      <td>Stark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.529421</td>\n",
       "      <td>4.037920</td>\n",
       "      <td>Stark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.101191</td>\n",
       "      <td>2.030075</td>\n",
       "      <td>Stark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.535178</td>\n",
       "      <td>1.790706</td>\n",
       "      <td>Stark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.817922</td>\n",
       "      <td>0.162836</td>\n",
       "      <td>Stark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y   Type\n",
       "0  2.759367  3.450856  Stark\n",
       "1  2.529421  4.037920  Stark\n",
       "2  6.101191  2.030075  Stark\n",
       "3  2.535178  1.790706  Stark\n",
       "4  4.817922  0.162836  Stark"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [4 + np.random.normal() for i in range(20)] + [2 + np.random.normal() for i in range(20)]\n",
    "y = [2 + np.random.normal() for i in range(20)] + [4 + np.random.normal() for i in range(20)]\n",
    "z = ['Stark'] * 20 + ['Bolton'] * 20\n",
    "data = pd.DataFrame({'x' : x, 'y' : y, 'Type' : z})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "racial-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_classification(line = False, point1 = False, point2 = False, point3 = False):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (13, 8))\n",
    "    \n",
    "    sns.scatterplot(data = data, x = 'x', y = 'y', hue = 'Type', ax = ax)\n",
    "    \n",
    "    if line:\n",
    "        ax.plot([0, data['x'].max()], [0, data['x'].max()])\n",
    "        \n",
    "    if point1:\n",
    "        ax.plot([4], [2], markersize = 15, marker = '*', color = 'g')\n",
    "        \n",
    "    if point2:\n",
    "        ax.plot([2], [4], markersize = 15, marker = '*', color = 'g')\n",
    "        \n",
    "    if point3:\n",
    "        ax.plot([4], [4], markersize = 15, marker = '*', color = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "private-grade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c320a3c59bf40b5b96f78f0e384c82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='line'), Checkbox(value=False, description='point1'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(linear_classification, line = False, point1 = False, point2 = False, point3 = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-observer",
   "metadata": {},
   "source": [
    "We can solve this problem by defining $h$ as follows:\n",
    "\n",
    "$$h(x, y) = x - y$$\n",
    "\n",
    "- If $h > 0$ → Stark\n",
    "- If $h < 0$ → Bolton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-semiconductor",
   "metadata": {},
   "source": [
    "# Biological inspiration\n",
    "\n",
    "\n",
    "This model, strange as it may seem, can be interpreted as a neuron, if we observe a neuron in our brain we will see that it is made up of several denditres and an axon, the neuron connects its axon to the denditres of another neuron, and in turn has several neurons connected to their own denditras.\n",
    "\n",
    "<img src = https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Blausen_0657_MultipolarNeuron.png/1024px-Blausen_0657_MultipolarNeuron.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-qualification",
   "metadata": {},
   "source": [
    "So a neuron receives information from several more neurons, but only sends a result (through the axon) to another, that is, the neuron has several inputs but only has one output. Like the previous model that receives two inputs ($x$, $y$) and a single output ($Stark$ / $Bolton$ depending on the case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-finland",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# From linear regression to logistics regression\n",
    "\n",
    "\n",
    "The first idea that we could have when trying to model the above would be to use the probability that a point is Stark or Bolton and use what we already know, linear regression to model it.\n",
    "\n",
    "$$P = \\alpha + \\beta X$$\n",
    "\n",
    "But something is wrong, here $P \\in [0, 1]$ but in the right side $X \\in (- \\infty, \\infty)$, so we still have a proiblem. What if we use the odds ratio?, the odds ratio is defined as follows: \n",
    "\n",
    "$$Odds_{P} = \\frac{P}{1 - P}$$\n",
    "\n",
    "if we use the odds ratio instead of probability our model would be as follows:\n",
    "\n",
    "$$\\frac{P}{1 - P} = \\alpha + \\beta X$$\n",
    "\n",
    "\n",
    "Notice when the numerator tends to 0 (the probability of success tends to 0) the odds ratio tends to 0. In the other hand when the numerator is bigger than the denominator, the odds ratio is bigger than one, even more so if the denominator tends to 0 (the probability of failure tends to 0 or the probability of success tends to 1) the odds ratio tend to $\\infty$. Thus the odds ratio is a real number in the $[0, \\infty)$, but in the right side of our equation we still have $X \\in (- \\infty, \\infty)$, the problem still there.\n",
    "\n",
    "\n",
    "So let's take a look at the function $\\ln$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea8267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "warming-norway",
   "metadata": {},
   "source": [
    "\n",
    "$$\\ln \\left| \\frac{P}{1 - P} \\right| =  \\alpha + \\beta X$$\n",
    "\n",
    "Finally $\\ln \\left| \\frac{P}{1 - P} \\right| \\in (- \\infty, \\infty)$ and $X \\in (- \\infty, \\infty)$ and:\n",
    "\n",
    "\n",
    "Maybe you're thinking that the $\\ln$ is not look like a straight line, and you are right, but don't forget the for of the form of the odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-clinton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-shakespeare",
   "metadata": {},
   "source": [
    "we can solve for P: \n",
    "\n",
    "$$\\ln \\left| \\frac{P}{1 - P} \\right|=  \\alpha + \\beta X$$\n",
    "\n",
    "\n",
    "Apply the exponential function:\n",
    "\n",
    "$$\\frac{P}{1 - P} = e^{\\alpha + \\beta X}$$\n",
    "\n",
    "\n",
    "multiply by $(1 - P)$\n",
    "\n",
    "$$P = e^{\\alpha + \\beta X} (1 - P)$$\n",
    "$$P = e^{\\alpha + \\beta X}  - P e^{\\alpha + \\beta X}$$\n",
    "\n",
    "\n",
    "Add $P e^{\\alpha + \\beta X}$\n",
    "\n",
    "$$P + P e^{\\alpha + \\beta X} = e^{\\alpha + \\beta X}$$\n",
    "\n",
    "$$P (1 + e^{\\alpha + \\beta X}) = e^{\\alpha + \\beta X}$$\n",
    "\n",
    "\n",
    "Divide by $(1 + e^{\\alpha + \\beta X})$\n",
    "\n",
    "$$P = \\frac{e^{\\alpha + \\beta X}}{1 + e^{\\alpha + \\beta X}}$$\n",
    "\n",
    "And finally apply a sneaky 1:\n",
    "\n",
    "$$P = \\frac{e^{-(\\alpha + \\beta X)}}{e^{-(\\alpha + \\beta X)}}\\frac{e^{\\alpha + \\beta X}}{1 + e^{\\alpha + \\beta X}}$$\n",
    "\n",
    "$$P = \\frac{1}{1 + e^{-(\\alpha + \\beta X)}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-miniature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "polar-stock",
   "metadata": {},
   "source": [
    "In the case of multiple predictor variables we would have:\n",
    "\n",
    "$$\\ln \\left| \\frac{P}{1 - P} \\right| =  w_0 + w_1 X_1 + \\cdots + w_k X_k$$\n",
    "\n",
    "And we can define two vectors:\n",
    "\n",
    "$$\\vec{w}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_0\\\\\n",
    "w_1\\\\\n",
    "\\vdots \\\\\n",
    "w_k \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\vec{X}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "X_1\\\\\n",
    "\\vdots \\\\\n",
    "X_k \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\ln \\left| \\frac{P}{1 - P} \\right| = w^T X$$\n",
    "\n",
    "$$P = \\frac{1}{1 + e^{-(w^T X)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-decade",
   "metadata": {},
   "source": [
    "# How we estimate the $\\vec{w}$\n",
    "\n",
    "Every time we fit a statistical or machine learning model, we are estimating parameters.A single variable linear regression has the equation:\n",
    "\n",
    "$Y = w_0 + w_1 X$\n",
    "\n",
    "Our goal when we fit this model is to estimate the parameters  $w_0$ and $w_1$ given our observed values of $Y$ and $X$. We use Ordinary Least Squares (OLS) to fit the linear regression model and estimate $w_0$ and $w_1$.\n",
    "\n",
    "- **Can we use OLS to estimate the $\\vec{w}$?**\n",
    "\n",
    "The answer is **No**. So we need to change cost function, now we will focus on the cross-error entropy function call.\n",
    "\n",
    "$$j = - \\left[y \\ln(P) + (1 - y) \\ln(1 - P) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dominican-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def j(y, p):\n",
    "    return - (y * np.log(p) + (1 - y) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tutorial-vision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j(1, 0.9) #rigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "funny-revision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.863232841258714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j(1, 0.021) #wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "revolutionary-iraqi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01005033585350145"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j(0, 0.01) #rigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b65536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j(0, 0.9) #wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-automation",
   "metadata": {},
   "source": [
    "As you can see, this metric is smaller the smaller the error we make with our prediction, so we can reuse the above to build a function to minimize.\n",
    "\n",
    "$$J = - \\frac{1}{n} \\sum \\left[y_i \\ln(P_i) + (1 - y_i) \\ln(1 - P_i) \\right]$$\n",
    "\n",
    "To minimize the previous function we can use the descent of the gradient but for this we need to know what the gradient is, but first we must remember that P is a function of $w^T X$ and this in turn of $w_i$.\n",
    "\n",
    "$$P = \\frac{1}{1 + e^{-(w^T X)}}$$\n",
    "\n",
    "\n",
    "So if we use the chain rule we can express the derivative of $J$ with respect to $w_i$ as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = \\sum \\frac{\\partial J}{\\partial P_i}\\frac{\\partial P_i}{\\partial \\alpha_i}\\frac{\\partial \\alpha_i}{\\partial w_i}$$\n",
    "\n",
    "where $\\alpha$ is:\n",
    "\n",
    "\n",
    "$$\\alpha_i = w^T x_i$$\n",
    "\n",
    "If we calculate each of the individual derivatives we have:\n",
    "\n",
    "$$\n",
    "\\left \\{\n",
    "\\begin{array}{l}\n",
    "\\frac{\\partial J}{\\partial P_i} = -\\left[\\frac{y_i}{p_i} - \\frac{1 - y_i}{1 - p_i}\\right]\\\\\n",
    "\\frac{\\partial P_i}{\\partial \\alpha_i} = \\frac{1}{(1 + e^{-(w^T X)})^2} (e^{-\\alpha_i})(- 1)\\\\\n",
    "\\frac{\\partial \\alpha_i}{\\partial w_i} = x_{ni}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The second derivative can be simplified\n",
    "\n",
    "$$\\frac{\\partial P_i}{\\partial \\alpha_i} = \\frac{1}{(1 + e^{-(w^T X)})^2} (e^{-\\alpha_i})(- 1)$$\n",
    "<hr>\n",
    "$$\\frac{1}{(1 + e^{-(w^T X)})^2} (e^{-\\alpha_i})(- 1) = \\frac{e^{-\\alpha_i}}{(1 + e^{-(w^T X)})^2}$$\n",
    "<hr>\n",
    "$$\\frac{e^{-\\alpha_i}}{(1 + e^{-(w^T X)})^2} = \\frac{1}{1 + e^{-(w^T X)}} \\frac{e^{-\\alpha_i}}{(1 + e^{-(w^T X)})}$$\n",
    "\n",
    "Finally notice that:\n",
    "\n",
    "$$1 - P_i = 1 - \\frac{1}{1 + e^{-(\\vec{\\beta} \\cdot \\vec{X}_{ui})}}$$\n",
    "\n",
    "$$1 - P_i = \\frac{1 + e^{-(\\vec{\\beta} \\cdot \\vec{X}_{ui})}}{1 + e^{-(\\vec{\\beta} \\cdot \\vec{X}_{ui})}} - \\frac{1}{1 + e^{-(\\vec{\\beta} \\cdot \\vec{X}_{ui})}}$$\n",
    "\n",
    "$$1 - P_i = \\frac{1 + e^{-(\\vec{\\beta} \\cdot \\vec{X}_{ui})} - 1}{1 + e^{-(\\vec{\\beta} \\cdot \\vec{X}_{ui})}}$$\n",
    "\n",
    "<div class=\"alert alert-danger\">$$1 - P_i = \\frac{e^{-(\\vec{\\beta} \\cdot \\vec{X}_{ui})}}{1 + e^{-(\\vec{\\beta} \\cdot \\vec{X}_{ui})}}$$</div>\n",
    "\n",
    "\n",
    "So.\n",
    "\n",
    "\n",
    "$$\n",
    "\\left \\{\n",
    "\\begin{array}{l}\n",
    "\\frac{\\partial J}{\\partial P_i} = -\\left[\\frac{y_i}{p_i} - \\frac{1 - y_i}{1 - p_i}\\right]\\\\\n",
    "\\frac{\\partial P_i}{\\partial \\alpha_i} = P_i (1 - P_i)\\\\\n",
    "\\frac{\\partial \\alpha_i}{\\partial w_i} = x_{ni}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Putting them all together:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = - \\sum \\left[ \\frac{y_i}{P_i} P_i(1 - P_i)x_{ni} - \\frac{1 - y_i}{1 - p_i} P_i (1 - P_i) x_{ni}\\right]$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = - \\sum \\left[ y_i(1 - P_i)x_{ni} - (1 - y_i) P_i  x_{ni}\\right]$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = - \\sum \\left[ (y_i - y_iP_i - P_i + y_iP_i)  x_{ni}\\right]$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = - \\sum(y_i  - P_i)  x_{ni}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} =  \\sum(P_i  - y_i)  x_{ni}$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\vec{w}} =  \\sum(P_i  - y_i)  \\vec{x_n}$$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\\nabla J =  X^T (P  - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "desirable-chancellor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.948864</td>\n",
       "      <td>5.043422</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.654946</td>\n",
       "      <td>3.221058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.346167</td>\n",
       "      <td>3.612457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.160796</td>\n",
       "      <td>4.624555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.705330</td>\n",
       "      <td>3.836523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y  Type\n",
       "0  3.948864  5.043422     1\n",
       "1  4.654946  3.221058     1\n",
       "2  2.346167  3.612457     1\n",
       "3  3.160796  4.624555     1\n",
       "4  3.705330  3.836523     1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [4 + np.random.normal() for i in range(20)] + [2 + np.random.normal() for i in range(20)]\n",
    "y = [4 + np.random.normal() for i in range(20)] + [2 + np.random.normal() for i in range(20)]\n",
    "z = [1] * 20 + [0] * 20\n",
    "data = pd.DataFrame({'x' : x, 'y' : y, 'Type' : z})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "random-baseline",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_18152/58723961.py, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\maria\\AppData\\Local\\Temp/ipykernel_18152/58723961.py\"\u001b[1;36m, line \u001b[1;32m48\u001b[0m\n\u001b[1;33m    model = LogisticRegressionGD(learning_rate=0.01, num_iterations=1000)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradiente descendente\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "            \n",
    "            # Cálculo de los gradientes\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Actualización de pesos y sesgo\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)\n",
    "    \n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    \n",
    "x = [4 + np.random.normal() for i in range(20)] + [2 + np.random.normal() for i in range(20)]\n",
    "y = [4 + np.random.normal() for i in range(20)] + [2 + np.random.normal() for i in range(20)]\n",
    "z = [1] * 20 + [0] * 20\n",
    "data = pd.DataFrame({'x' : x, 'y' : y, 'Type' : z})\n",
    "data.head()\n",
    "    \n",
    "    # Instanciar y entrenar el modelo\n",
    "    model = LogisticRegressionGD(learning_rate=0.01, num_iterations=1000)\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    # Predecir y mostrar los resultados\n",
    "    y_pred = model.predict(x)\n",
    "    print(\"Valores reales:\", y)\n",
    "    print(\"Valores predichos:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-composition",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "implement logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __j(self,*w):\n",
    "        a=self.y*np.log(self.__sigmoid(self.X @self.w))\n",
    "        b=(1-self.y)*np.log(1-self.__sigmoid(self.X@self.w))\n",
    "    def __sigmoid(self,x):\n",
    "        return 1 /(1+np.exp(-x))\n",
    "         \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d63ea",
   "metadata": {},
   "source": [
    "## Performance Measures\n",
    "\n",
    "<img src = 'https://geekflare.com/wp-content/uploads/2022/07/basic_cm-edited.jpg'>\n",
    "\n",
    "\n",
    "True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n",
    "\n",
    "True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n",
    "\n",
    "False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n",
    "\n",
    "False Positives (FP) – When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n",
    "\n",
    "False Negatives (FN) – When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n",
    "\n",
    "Once you understand these four parameters then we can calculate Accuracy, Precision, Recall and F1 score.\n",
    "\n",
    "\n",
    "**Accuracy** - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model.\n",
    "\n",
    "\n",
    "**Precision** - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. \n",
    "\n",
    "**Recall (Sensitivity)** - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label?\n",
    "\n",
    "**F1 score** - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
